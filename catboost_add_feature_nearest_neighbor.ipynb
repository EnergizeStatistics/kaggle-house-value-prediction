{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zillow's Home Value Prediction (Zestimate) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import geopy.distance\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/lee/Documents/Datasets for GitHub/kaggle_zillow_home_value_prediction/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_hash_labeled_clean_train = pd.read_pickle(data_dir+'df_no_hash_labeled_clean_train.pkl')\n",
    "# df_labeled_raw_train = pd.read_pickle(data_dir+'df_labeled_raw_train.pkl')\n",
    "# df_labeled_raw_val = pd.read_pickle(data_dir+'df_labeled_raw_val.pkl')\n",
    "\n",
    "# df_labeled_raw_train['latitude'] = df_labeled_raw_train['latitude'] / 1e6\n",
    "# df_labeled_raw_train['longitude'] = df_labeled_raw_train['longitude'] / 1e6\n",
    "\n",
    "# df_labeled_raw_val['latitude'] = df_labeled_raw_val['latitude'] / 1e6\n",
    "# df_labeled_raw_val['longitude'] = df_labeled_raw_val['longitude'] / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Features ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The price of a house is highly correlated to other houses near it. For each house in the training dataset, we identify $n$ nearest neighbors in terms of GPS coordinates. Then we average their tax values.\n",
    "\n",
    "There is more than one way to implement this approach. The `geopy` package calculates distance but is computationally inhibitive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_closest_houses(cord_input, df, n_neighbors=10):\n",
    "#     # find the distance between given coordinates and all coordinates in the dataset\n",
    "#     dist = df.apply(lambda row: geopy.distance.distance(cord_input, (row['latitude'], row['longitude'])).km, axis=1)\n",
    "    \n",
    "#     # find the nearest neighbors, remove the house at the given coordinates itself\n",
    "#     closest_parcelid = dist.nsmallest(n_neighbors)\n",
    "#     closest_parcelid = closest_parcelid[closest_parcelid > 0].copy()\n",
    "    \n",
    "#     # the average log error of these nearest neighbors\n",
    "#     avg_log_error = df.loc[closest_parcelid.index, 'logerror'].mean()\n",
    "    \n",
    "#     gc.collect()\n",
    "    \n",
    "#     return avg_log_error\n",
    "\n",
    "# start = time.time()\n",
    "# candidate_prices = df_labeled_raw_train[['logerror', 'latitude', 'longitude']].dropna().copy()\n",
    "# candidate_prices['closest_neighbors_logerror'] = candidate_prices.apply(lambda row: \\\n",
    "#                                                    find_closest_houses((row['latitude'], row['longitude']), \\\n",
    "#                                                                        candidate_prices, \\\n",
    "#                                                                        n_neighbors=20), axis=1)\n",
    "# print(\"Took {:.8f} s\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_hash_labeled_clean_train.sort_values('transactiondate', inplace=True)\n",
    "num_neighbors = 3\n",
    "train_dates = df_no_hash_labeled_clean_train.transactiondate.unique()[1:]\n",
    "# creating some columns that we will be populating\n",
    "\n",
    "# df_labeled_raw_train['avg_nn_logerror'] = np.nan\n",
    "df_no_hash_labeled_clean_train['avg_nn_taxvaluedollarcnt'] = np.nan\n",
    "df_no_hash_labeled_clean_train['avg_nn_structuretaxvaluedollarcnt'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Nearest Neighbors ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_int_id = df_no_hash_labeled_clean_train[df_no_hash_labeled_clean_train['latitude'].notnull() & df_no_hash_labeled_clean_train['longitude'].notnull()].reset_index()\n",
    "\n",
    "for d in train_dates:\n",
    "    previous_transactions = train_int_id.loc[train_int_id.transactiondate < d, ['longitude','latitude']]\n",
    "    current_date_transactions = train_int_id.loc[train_int_id.transactiondate == d, ['longitude','latitude']]\n",
    "    if previous_transactions.shape[0] >= num_neighbors: \n",
    "        nbrs = NearestNeighbors(n_neighbors=num_neighbors, algorithm='ball_tree')\\\n",
    "                .fit(previous_transactions)\n",
    "        distances, indices = nbrs.kneighbors(current_date_transactions)\n",
    "        indices = indices.reshape(-1, 1)\n",
    "\n",
    "#         nearest_logerrors = train_int_id.loc[indices.squeeze(), 'logerror']\n",
    "#         nearest_logerrors = nearest_logerrors.values.reshape(-1, num_neighbors)\n",
    "\n",
    "        nearest_taxvaluedollarcnt = train_int_id.loc[indices.squeeze(), 'taxvaluedollarcnt']\n",
    "        nearest_taxvaluedollarcnt = nearest_taxvaluedollarcnt.values.reshape(-1, num_neighbors)\n",
    "\n",
    "        nearest_structuretaxvaluedollarcnt = train_int_id.loc[indices.squeeze(), 'structuretaxvaluedollarcnt']\n",
    "        nearest_structuretaxvaluedollarcnt = nearest_structuretaxvaluedollarcnt.values.reshape(-1, num_neighbors)\n",
    "\n",
    "#         train_int_id.loc[train_int_id.transactiondate==d, ['avg_nn_logerror']] = nearest_logerrors.mean(axis=1)\n",
    "        train_int_id.loc[train_int_id.transactiondate == d, ['avg_nn_taxvaluedollarcnt']] = nearest_taxvaluedollarcnt.mean(axis=1)\n",
    "        train_int_id.loc[train_int_id.transactiondate == d, ['avg_nn_structuretaxvaluedollarcnt']] = nearest_structuretaxvaluedollarcnt.mean(axis=1)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_int_id.set_index('parcelid', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_int_id_ms = df_no_hash_labeled_clean_train[df_no_hash_labeled_clean_train['latitude'].isnull() | df_no_hash_labeled_clean_train['longitude'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled_train = pd.concat([train_int_id, train_int_id_ms])\n",
    "del train_int_id, train_int_id_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_all_rm_miss = tuple(df_labeled_train.drop(['logerror', 'transactiondate'], axis=1))\n",
    "\n",
    "flag_features_set = set(('fireplaceflag', 'hashottuborspa', 'pooltypeid10', 'pooltypeid2', 'pooltypeid7', \\\n",
    "                         'taxdelinquencyflag'))\n",
    "\n",
    "categorical_features_set = set(('airconditioningtypeid', 'architecturalstyletypeid', \\\n",
    "                                'buildingclasstypeid', 'decktypeid', 'fips', 'heatingorsystemtypeid', \\\n",
    "                                'propertycountylandusecode', 'propertylandusetypeid', \\\n",
    "                                'propertyzoningdesc', 'rawcensustractandblock', 'censustractandblock', \\\n",
    "                                'regionidcounty', 'regionidcity', 'regionidzip', \\\n",
    "                                'regionidneighborhood', 'typeconstructiontypeid', 'assessmentyear', \\\n",
    "                                'taxdelinquencyyear', 'transaction_year', 'transaction_month'))\n",
    "\n",
    "categorical_features_index_rm_miss = list(icol for icol, col in enumerate(columns_all_rm_miss) \\\n",
    "                                          if (col in categorical_features_set) == True)\n",
    "\n",
    "categorical_features_names_rm_miss = tuple(col for col in columns_all_rm_miss \\\n",
    "                                           if (col in categorical_features_set) == True)\n",
    "\n",
    "# del flag_features_set, continuous_features_set, categorical_features_set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labeled_train = pd.read_pickle(data_dir+'y_labeled_train.pkl')\n",
    "X_no_hash_labeled_train = df_labeled_train.drop(['logerror', 'transactiondate'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_no_hash_labeled_train[list(categorical_features_names_rm_miss)] = X_no_hash_labeled_train[list(categorical_features_names_rm_miss)]\\\n",
    "                                                            .astype(str)\n",
    "# initialize Pool\n",
    "train_pool = Pool(X_no_hash_labeled_train, label=y_labeled_train, cat_features=categorical_features_index_rm_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_catboost = CatBoostRegressor(loss_function='MAE', eval_metric='MAE', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "reg_catboost.fit(train_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE in training: 0.06816847\n"
     ]
    }
   ],
   "source": [
    "# make the prediction using the resulting model\n",
    "print(\"MAE in training: {:.8f}\".format(mean_absolute_error(y_labeled_train, reg_catboost.predict(train_pool))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new features do not improve training MAE. Go back to the model without these features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
